{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaKumari500092077/Machine-Learning-Lab/blob/main/Session_9_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 9**: Random Forest Regression\n",
        "\n",
        "Objective: This assignment provides a hands-on understanding of the Random Forest Regression algorithm. You will implement the algorithm from scratch and compare its performance with the scikit-learn implementation. You will also tune hyperparameters using cross-validation.\n",
        "\n",
        "Dataset:  Diabetes dataset (https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)\n",
        "\n",
        "Tasks:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOAUAGxS9cO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preparation: [1 Marks]\n",
        "*   Load the diabetes dataset using sklearn.datasets.load_diabetes.\n",
        "*   Split the dataset into training and test sets with a 80%-20% ratio."
      ],
      "metadata": {
        "id": "9b8gX_6s9ppz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "diabetes_data = load_diabetes()\n",
        "X = diabetes_data.data\n",
        "Y = diabetes_data.target\n",
        "dataset = pd.DataFrame(data=np.c_[X, Y], columns=diabetes_data.feature_names + ['target'])\n",
        "print(dataset.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNJnFDQrJ2B2",
        "outputId": "47233d2f-57ff-422f-e6ff-6feafb4521d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 442 entries, 0 to 441\n",
            "Data columns (total 11 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   age     442 non-null    float64\n",
            " 1   sex     442 non-null    float64\n",
            " 2   bmi     442 non-null    float64\n",
            " 3   bp      442 non-null    float64\n",
            " 4   s1      442 non-null    float64\n",
            " 5   s2      442 non-null    float64\n",
            " 6   s3      442 non-null    float64\n",
            " 7   s4      442 non-null    float64\n",
            " 8   s5      442 non-null    float64\n",
            " 9   s6      442 non-null    float64\n",
            " 10  target  442 non-null    float64\n",
            "dtypes: float64(11)\n",
            "memory usage: 38.1 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfYLHzkyEDs7",
        "outputId": "74ca5ef4-b084-489e-c087-2a2fc9e34822"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        age       sex       bmi        bp        s1        s2        s3  \\\n",
            "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
            "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
            "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
            "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
            "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
            "\n",
            "         s4        s5        s6  target  \n",
            "0 -0.002592  0.019907 -0.017646   151.0  \n",
            "1 -0.039493 -0.068332 -0.092204    75.0  \n",
            "2 -0.002592  0.002861 -0.025930   141.0  \n",
            "3  0.034309  0.022688 -0.009362   206.0  \n",
            "4 -0.002592 -0.031988 -0.046641   135.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for duplicates\n",
        "duplicates_value = dataset.duplicated().sum()\n",
        "print(f'No. of Duplicates: {duplicates_value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WteBqL07A_Ac",
        "outputId": "3ab85262-189b-4f5a-a562-ff6a63dd04e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of Duplicates: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for missing values\n",
        "missing_value = dataset.isnull().sum()\n",
        "print(f'Total Number of Missing Values: {missing_value.sum()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnjHWv8NBYTI",
        "outputId": "74654107-cf1c-43fa-edbc-09a54b8a697e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Missing Values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "dataset = sc.fit_transform(dataset)\n",
        "\n",
        "# convert scaled data back to dataframe\n",
        "dataset = pd.DataFrame(dataset, columns=diabetes_data.feature_names + ['target'])"
      ],
      "metadata": {
        "id": "lbcN03e4KiI_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into traing and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = dataset.drop(columns='target')\n",
        "Y = dataset['target']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_train = X_train.to_numpy()\n",
        "X_test = X_test.to_numpy()\n",
        "Y_train = Y_train.to_numpy()\n",
        "Y_test = Y_test.to_numpy()\n"
      ],
      "metadata": {
        "id": "fseYdtUWCIbf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. From-Scratch Implementation: [7 Marks]\n",
        "* Implement the Random Forest Regression algorithm from scratch in Python."
      ],
      "metadata": {
        "id": "cfJrY9Ms9wmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeRegression:\n",
        "\n",
        "  def __init__(self, max_depth=5, max_features='sqrt'):\n",
        "    self.max_depth = max_depth # Maximum depth of the tree\n",
        "    self.max_features = max_features # Number of features to consider for each split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    self.no_of_features = X.shape[1] # Number of features in the dataset\n",
        "    if(self.max_features == 'sqrt'):\n",
        "      self.max_features = int(np.sqrt(self.no_of_features)) # Calculate the number of features to use\n",
        "    self.tree_ = self._build_tree(X, Y)  # Build the tree\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def _build_tree(self, X, Y, depth = 0):\n",
        "    no_of_samples, no_of_features = X.shape # Get the number of samples and features\n",
        "    if(depth >= self.max_depth or no_of_samples < 2): # Stopping criteria: Maximum depth reached, Not enough samples to split\n",
        "      return np.mean(Y) # Return the mean of the target values\n",
        "\n",
        "    # Select random subset of features\n",
        "    feature_indices = np.random.choice(no_of_features, size=self.max_features, replace=False)\n",
        "\n",
        "    # Select best split from random subset\n",
        "    best_mean_squared_error = float('inf')\n",
        "    best_feature_index = None\n",
        "    best_threshold = None\n",
        "\n",
        "    for index in feature_indices: # Iterate through selected feature\n",
        "      sorted_indices = np.argsort(X[:, index])  # Get indices that would sort the feature values\n",
        "      thresholds = X[sorted_indices, index]  # Sort the thresholds\n",
        "      values = Y[sorted_indices]  # Sort the corresponding target values\n",
        "\n",
        "      for i in range(1, no_of_samples):\n",
        "         if(values[i - 1] != values[i]): # Check if the values changes between consecutive samples\n",
        "          thr = (thresholds[i - 1] + thresholds[i]) / 2\n",
        "          left = [Y[j] for j in range(no_of_samples) if X[j, index] < thr] # Samples in the left child\n",
        "          right = [Y[j] for j in range(no_of_samples) if X[j, index] >= thr] # Samples in the right child\n",
        "          # Handle empty or single-element arrays\n",
        "          var_left = np.var(left) if len(left) > 1 else 0\n",
        "          var_right = np.var(right) if len(right) > 1 else 0\n",
        "\n",
        "          # Check for zero division\n",
        "          if no_of_samples > 0:\n",
        "            mean_square_error = ((len(left) * var_left) + (len(right) * var_right)) / no_of_samples\n",
        "          #else:\n",
        "           # mean_square_error = 0\n",
        "\n",
        "\n",
        "          if mean_square_error < best_mean_squared_error: # Check if this split is better\n",
        "            best_mean_squared_error = mean_square_error\n",
        "            best_feature_index = index\n",
        "            best_threshold = thr\n",
        "\n",
        "    if(best_mean_squared_error == float('inf')): # If no split found\n",
        "      return np.mean(Y) # Return mean as leaf value\n",
        "\n",
        "    # Recur to build the left and right subtrees\n",
        "    left_indexes = np.where(X[:, best_feature_index] < best_threshold) # Indices of samples in the left child\n",
        "    right_indexes = np.where(X[:, best_feature_index] >= best_threshold) # Indices of samples in the right child\n",
        "    left = self._build_tree(X[left_indexes], Y[left_indexes], depth + 1) # Recursively build left subtree\n",
        "    right = self._build_tree(X[right_indexes], Y[right_indexes], depth + 1) # Recursively build right subtree\n",
        "    return (best_feature_index, best_threshold, left, right) # Return the node (feature index, threshold, left child, right child)\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self, X): # predicts the class labels for a given feature matrix X\n",
        "        return [self._predict(inputs) for inputs in X] # Predict for each sample in X\n",
        "\n",
        "\n",
        "\n",
        "  def _predict(self, inputs): # traverses the tree to make a prediction for a single sample.\n",
        "    node = self.tree_ # Start at the root node\n",
        "    while isinstance(node, tuple):\n",
        "      index, thr, left, right = node\n",
        "      if inputs[index] < thr:\n",
        "        node = left\n",
        "      else:\n",
        "        node = right\n",
        "    return node  # Return the mean value at the leaf node\n",
        "\n",
        "\n",
        "\n",
        "class RandomForestRegression:\n",
        "\n",
        "  def __init__(self, no_of_trees=100, max_depth=5, max_features='sqrt'):\n",
        "    self.no_of_trees = no_of_trees # Number of trees in the forest\n",
        "    self.max_depth = max_depth # Maximum depth of each tree\n",
        "    self.max_features = max_features # Number of features to consider for each split\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    self.trees = []  # List to store the decision trees\n",
        "    self.indices = [] # List to store the indices used for bootstrapping\n",
        "    no_of_samples = X.shape[0] # Number of samples\n",
        "    for _ in range(self.no_of_trees): # Iterate to create no_of_trees\n",
        "      tree = DecisionTreeRegression(max_depth=self.max_depth, max_features=self.max_features) # Create a DecisionTree\n",
        "      indices = np.random.choice(no_of_samples, size=no_of_samples, replace=True) # Bootstrap samples\n",
        "      tree.fit(X[indices], Y[indices]) # Train the tree on bootstrapped samples; For each tree, it generates a bootstrap sample (random sampling with replacement)\n",
        "      self.trees.append(tree) # Add the trained tree to the list\n",
        "      self.indices.append(indices) # Store the indices used for bootstrapping\n",
        "\n",
        "  def predict(self, X):\n",
        "    # Get predictions from each tree and return their mean\n",
        "    predictions = np.array([tree.predict(X) for tree in self.trees]) # Get predictions from each tree\n",
        "    return np.mean(predictions, axis=0) # Return the mean of the predictions\n",
        "\n",
        "\n",
        "\n",
        "def calculate_oob_error(X, y, forest):\n",
        "    no_of_samples = X.shape[0] # Number of samples\n",
        "    no_of_trees = forest.no_of_trees # Number of trees\n",
        "    predictions = np.zeros((no_of_samples, no_of_trees)) # Array to store OOB predictions\n",
        "\n",
        "\n",
        "    for i, tree in enumerate(forest.trees): # Iterate over trees\n",
        "        indices = forest.indices[i] # Indices used to train the current tree\n",
        "        oob_indices = np.array([index for index in range(no_of_samples) if index not in indices]) # Out-of-bag indices\n",
        "        if len(oob_indices) > 0: # If there are OOB samples\n",
        "          predictions[oob_indices, i] = tree.predict(X[oob_indices]) # Predict for OOB samples\n",
        "\n",
        "\n",
        "    # Calculate the mean prediction for each sample\n",
        "    oob_predictions = np.mean(predictions, axis=1) # Average predictions along the tree axis\n",
        "\n",
        "    # Calculate Mean Squared Error (MSE) only for samples with predictions\n",
        "    valid_indices = ~np.isnan(oob_predictions)  # Find indices where predictions are valid\n",
        "    oob_err = np.mean((y[valid_indices] - oob_predictions[valid_indices]) ** 2) if np.any(valid_indices) else None\n",
        "\n",
        "    return oob_err  # Return the mean squared error as the OOB error\n"
      ],
      "metadata": {
        "id": "GosVQsCG9BFE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train Random Forest with feature randomness\n",
        "rf = RandomForestRegression(no_of_trees=100, max_depth=5, max_features='sqrt')\n",
        "rf.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate out-of-bag error\n",
        "oob_err = calculate_oob_error(X_train, Y_train, rf)\n",
        "print(\"Out-of-Bag Error:\", oob_err)\n",
        "\n",
        "# Predict on the test set\n",
        "Y_pred = rf.predict(X_test)\n",
        "\n",
        "# Calculate regression metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "mean_squared_error = mean_squared_error(Y_test, Y_pred)  # Mean Squared Error\n",
        "print(\"Mean Squared Error by scratch implementation:\", mean_squared_error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaZf7cLb9kKk",
        "outputId": "c54c5ec2-0b4b-43b1-b8c0-17734e804ad7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag Error: 0.7632908033959095\n",
            "Mean Squared Error by scratch implementation: 0.47857137332314037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to plot a decision tree in a regression context\n",
        "def plot_regression_tree(tree, feature_names, depth=0):\n",
        "    \"\"\"\n",
        "    Recursively prints the structure of the decision tree, adapted for regression.\n",
        "    Displays predicted values at leaf nodes and splits at decision nodes.\n",
        "    \"\"\"\n",
        "    if isinstance(tree, tuple):\n",
        "        idx, thr, left, right = tree\n",
        "        feature_name = feature_names[idx]\n",
        "        # Print the condition at the decision node\n",
        "        print('  ' * depth + f'if {feature_name} <= {thr:.2f}:')  # Display the threshold value with 2 decimal precision\n",
        "        plot_regression_tree(left, feature_names, depth + 1)      # Recursive call for left subtree\n",
        "        print('  ' * depth + f'else:')\n",
        "        plot_regression_tree(right, feature_names, depth + 1)     # Recursive call for right subtree\n",
        "    else:\n",
        "        # Leaf node: display the predicted average value for regression\n",
        "        print('  ' * depth + f'Predicted value: {tree:.2f}')       # Display the prediction with 2 decimal precision\n",
        "\n",
        "# Create a list of feature names for visualization\n",
        "feature_names = [f'Feature {i}' for i in range(X_train.shape[1])]\n",
        "\n",
        "# Select a few decision trees for visualization\n",
        "trees_to_visualize = [0, 1, 2, 3, 4]  # Adjust the number of trees you want to visualize\n",
        "\n",
        "# Visualize each selected decision tree\n",
        "for i in trees_to_visualize:\n",
        "    print(f\"\\nDecision Tree {i+1}:\")\n",
        "    plot_regression_tree(rf.trees[i].tree_, feature_names)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DulVxcUjFQ0U",
        "outputId": "486d1979-b53d-49ac-ebf6-c71a3da76bc4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree 1:\n",
            "if Feature 8 <= -0.02:\n",
            "  if Feature 8 <= -0.91:\n",
            "    if Feature 6 <= 1.06:\n",
            "      if Feature 8 <= -1.10:\n",
            "        if Feature 7 <= -0.02:\n",
            "          Predicted value: -0.71\n",
            "        else:\n",
            "          Predicted value: 0.14\n",
            "      else:\n",
            "        if Feature 6 <= 0.56:\n",
            "          Predicted value: -1.31\n",
            "        else:\n",
            "          Predicted value: -0.81\n",
            "    else:\n",
            "      if Feature 0 <= -1.30:\n",
            "        Predicted value: -1.18\n",
            "      else:\n",
            "        if Feature 0 <= 0.69:\n",
            "          Predicted value: -1.35\n",
            "        else:\n",
            "          Predicted value: -1.47\n",
            "  else:\n",
            "    if Feature 2 <= 0.15:\n",
            "      if Feature 4 <= -1.23:\n",
            "        if Feature 8 <= -0.42:\n",
            "          Predicted value: 0.14\n",
            "        else:\n",
            "          Predicted value: -0.77\n",
            "      else:\n",
            "        if Feature 5 <= 1.78:\n",
            "          Predicted value: -0.81\n",
            "        else:\n",
            "          Predicted value: 1.31\n",
            "    else:\n",
            "      if Feature 4 <= -1.07:\n",
            "        Predicted value: 0.75\n",
            "      else:\n",
            "        if Feature 8 <= -0.56:\n",
            "          Predicted value: 0.95\n",
            "        else:\n",
            "          Predicted value: -0.33\n",
            "else:\n",
            "  if Feature 2 <= 1.17:\n",
            "    if Feature 2 <= -0.44:\n",
            "      if Feature 2 <= -0.70:\n",
            "        if Feature 7 <= 0.71:\n",
            "          Predicted value: 0.48\n",
            "        else:\n",
            "          Predicted value: -0.56\n",
            "      else:\n",
            "        if Feature 6 <= -0.45:\n",
            "          Predicted value: 0.08\n",
            "        else:\n",
            "          Predicted value: -0.75\n",
            "    else:\n",
            "      if Feature 3 <= 0.35:\n",
            "        if Feature 8 <= 1.36:\n",
            "          Predicted value: 0.03\n",
            "        else:\n",
            "          Predicted value: 0.87\n",
            "      else:\n",
            "        if Feature 6 <= 0.83:\n",
            "          Predicted value: 1.06\n",
            "        else:\n",
            "          Predicted value: -0.33\n",
            "  else:\n",
            "    if Feature 5 <= 1.34:\n",
            "      if Feature 3 <= -0.88:\n",
            "        Predicted value: 2.52\n",
            "      else:\n",
            "        if Feature 3 <= 1.51:\n",
            "          Predicted value: 1.49\n",
            "        else:\n",
            "          Predicted value: 2.00\n",
            "    else:\n",
            "      if Feature 2 <= 1.47:\n",
            "        Predicted value: -0.40\n",
            "      else:\n",
            "        if Feature 4 <= 2.08:\n",
            "          Predicted value: 0.36\n",
            "        else:\n",
            "          Predicted value: 0.88\n",
            "\n",
            "\n",
            "Decision Tree 2:\n",
            "if Feature 2 <= 0.11:\n",
            "  if Feature 6 <= -0.33:\n",
            "    if Feature 8 <= -0.51:\n",
            "      if Feature 5 <= -0.10:\n",
            "        if Feature 3 <= -0.84:\n",
            "          Predicted value: -0.84\n",
            "        else:\n",
            "          Predicted value: 0.44\n",
            "      else:\n",
            "        if Feature 0 <= 0.65:\n",
            "          Predicted value: -1.26\n",
            "        else:\n",
            "          Predicted value: -0.90\n",
            "    else:\n",
            "      if Feature 5 <= -0.19:\n",
            "        if Feature 2 <= -0.53:\n",
            "          Predicted value: 0.33\n",
            "        else:\n",
            "          Predicted value: -0.51\n",
            "      else:\n",
            "        if Feature 4 <= -0.31:\n",
            "          Predicted value: 1.51\n",
            "        else:\n",
            "          Predicted value: 0.41\n",
            "  else:\n",
            "    if Feature 0 <= 0.61:\n",
            "      if Feature 3 <= 0.82:\n",
            "        if Feature 1 <= 0.06:\n",
            "          Predicted value: -0.61\n",
            "        else:\n",
            "          Predicted value: -1.16\n",
            "      else:\n",
            "        if Feature 9 <= -0.24:\n",
            "          Predicted value: 0.85\n",
            "        else:\n",
            "          Predicted value: -0.51\n",
            "    else:\n",
            "      if Feature 8 <= -0.24:\n",
            "        if Feature 5 <= 0.46:\n",
            "          Predicted value: -0.59\n",
            "        else:\n",
            "          Predicted value: 0.16\n",
            "      else:\n",
            "        if Feature 9 <= -1.11:\n",
            "          Predicted value: -0.42\n",
            "        else:\n",
            "          Predicted value: 0.33\n",
            "else:\n",
            "  if Feature 7 <= -0.36:\n",
            "    if Feature 8 <= -0.78:\n",
            "      if Feature 8 <= -1.32:\n",
            "        Predicted value: -0.13\n",
            "      else:\n",
            "        if Feature 5 <= 0.06:\n",
            "          Predicted value: -1.29\n",
            "        else:\n",
            "          Predicted value: -1.20\n",
            "    else:\n",
            "      if Feature 2 <= 0.19:\n",
            "        if Feature 7 <= -0.81:\n",
            "          Predicted value: -0.56\n",
            "        else:\n",
            "          Predicted value: 0.39\n",
            "      else:\n",
            "        if Feature 9 <= -0.68:\n",
            "          Predicted value: -0.11\n",
            "        else:\n",
            "          Predicted value: 1.39\n",
            "  else:\n",
            "    if Feature 5 <= 0.79:\n",
            "      if Feature 8 <= -0.01:\n",
            "        if Feature 5 <= -0.40:\n",
            "          Predicted value: -0.54\n",
            "        else:\n",
            "          Predicted value: 0.62\n",
            "      else:\n",
            "        if Feature 2 <= 1.83:\n",
            "          Predicted value: 1.33\n",
            "        else:\n",
            "          Predicted value: 1.84\n",
            "    else:\n",
            "      if Feature 5 <= 3.73:\n",
            "        if Feature 2 <= 1.65:\n",
            "          Predicted value: 0.46\n",
            "        else:\n",
            "          Predicted value: 1.76\n",
            "      else:\n",
            "        Predicted value: -0.88\n",
            "\n",
            "\n",
            "Decision Tree 3:\n",
            "if Feature 3 <= 0.38:\n",
            "  if Feature 6 <= -0.41:\n",
            "    if Feature 2 <= 1.37:\n",
            "      if Feature 7 <= 0.93:\n",
            "        if Feature 4 <= -0.25:\n",
            "          Predicted value: -0.13\n",
            "        else:\n",
            "          Predicted value: 0.60\n",
            "      else:\n",
            "        if Feature 8 <= 1.34:\n",
            "          Predicted value: -0.49\n",
            "        else:\n",
            "          Predicted value: 0.89\n",
            "    else:\n",
            "      if Feature 4 <= 2.22:\n",
            "        if Feature 3 <= -0.47:\n",
            "          Predicted value: 1.95\n",
            "        else:\n",
            "          Predicted value: 0.96\n",
            "      else:\n",
            "        Predicted value: -0.40\n",
            "  else:\n",
            "    if Feature 8 <= -0.21:\n",
            "      if Feature 2 <= 0.21:\n",
            "        if Feature 0 <= 0.57:\n",
            "          Predicted value: -0.84\n",
            "        else:\n",
            "          Predicted value: -0.32\n",
            "      else:\n",
            "        if Feature 2 <= 0.47:\n",
            "          Predicted value: 0.86\n",
            "        else:\n",
            "          Predicted value: -0.25\n",
            "    else:\n",
            "      if Feature 9 <= 0.46:\n",
            "        if Feature 2 <= -0.54:\n",
            "          Predicted value: -0.73\n",
            "        else:\n",
            "          Predicted value: -0.04\n",
            "      else:\n",
            "        if Feature 5 <= 1.92:\n",
            "          Predicted value: 0.25\n",
            "        else:\n",
            "          Predicted value: 0.95\n",
            "else:\n",
            "  if Feature 9 <= 0.80:\n",
            "    if Feature 8 <= 0.34:\n",
            "      if Feature 4 <= -0.34:\n",
            "        if Feature 7 <= -0.38:\n",
            "          Predicted value: 0.01\n",
            "        else:\n",
            "          Predicted value: 1.55\n",
            "      else:\n",
            "        if Feature 4 <= -0.22:\n",
            "          Predicted value: -1.20\n",
            "        else:\n",
            "          Predicted value: -0.22\n",
            "    else:\n",
            "      if Feature 5 <= -0.97:\n",
            "        if Feature 7 <= 0.72:\n",
            "          Predicted value: 1.67\n",
            "        else:\n",
            "          Predicted value: 1.57\n",
            "      else:\n",
            "        if Feature 5 <= 0.99:\n",
            "          Predicted value: 0.90\n",
            "        else:\n",
            "          Predicted value: 0.05\n",
            "  else:\n",
            "    if Feature 4 <= -0.24:\n",
            "      if Feature 0 <= 0.65:\n",
            "        if Feature 7 <= 0.26:\n",
            "          Predicted value: 1.62\n",
            "        else:\n",
            "          Predicted value: 1.04\n",
            "      else:\n",
            "        if Feature 8 <= 0.32:\n",
            "          Predicted value: 1.62\n",
            "        else:\n",
            "          Predicted value: 2.16\n",
            "    else:\n",
            "      if Feature 4 <= -0.12:\n",
            "        Predicted value: -0.27\n",
            "      else:\n",
            "        if Feature 6 <= 0.60:\n",
            "          Predicted value: 1.04\n",
            "        else:\n",
            "          Predicted value: -0.87\n",
            "\n",
            "\n",
            "Decision Tree 4:\n",
            "if Feature 8 <= -0.02:\n",
            "  if Feature 5 <= 0.36:\n",
            "    if Feature 3 <= 0.97:\n",
            "      if Feature 4 <= -0.83:\n",
            "        if Feature 4 <= -0.94:\n",
            "          Predicted value: -0.67\n",
            "        else:\n",
            "          Predicted value: 0.05\n",
            "      else:\n",
            "        if Feature 8 <= -0.71:\n",
            "          Predicted value: -1.11\n",
            "        else:\n",
            "          Predicted value: -0.58\n",
            "    else:\n",
            "      if Feature 9 <= -0.07:\n",
            "        if Feature 2 <= 1.21:\n",
            "          Predicted value: 0.86\n",
            "        else:\n",
            "          Predicted value: 1.39\n",
            "      else:\n",
            "        if Feature 2 <= -0.90:\n",
            "          Predicted value: -1.05\n",
            "        else:\n",
            "          Predicted value: -0.47\n",
            "  else:\n",
            "    if Feature 2 <= -0.19:\n",
            "      if Feature 8 <= -0.46:\n",
            "        if Feature 6 <= -1.03:\n",
            "          Predicted value: -0.85\n",
            "        else:\n",
            "          Predicted value: -0.23\n",
            "      else:\n",
            "        if Feature 3 <= 0.50:\n",
            "          Predicted value: -1.24\n",
            "        else:\n",
            "          Predicted value: -1.13\n",
            "    else:\n",
            "      if Feature 5 <= 1.05:\n",
            "        if Feature 3 <= -0.12:\n",
            "          Predicted value: 1.46\n",
            "        else:\n",
            "          Predicted value: 0.63\n",
            "      else:\n",
            "        if Feature 3 <= 1.22:\n",
            "          Predicted value: -0.18\n",
            "        else:\n",
            "          Predicted value: -1.16\n",
            "else:\n",
            "  if Feature 8 <= 0.55:\n",
            "    if Feature 6 <= -1.49:\n",
            "      if Feature 3 <= 0.93:\n",
            "        Predicted value: 2.45\n",
            "      else:\n",
            "        Predicted value: 1.36\n",
            "    else:\n",
            "      if Feature 6 <= 0.56:\n",
            "        if Feature 3 <= 0.35:\n",
            "          Predicted value: -0.08\n",
            "        else:\n",
            "          Predicted value: 0.75\n",
            "      else:\n",
            "        if Feature 2 <= -0.99:\n",
            "          Predicted value: 0.23\n",
            "        else:\n",
            "          Predicted value: -0.75\n",
            "  else:\n",
            "    if Feature 0 <= -1.07:\n",
            "      if Feature 9 <= -0.33:\n",
            "        Predicted value: -1.09\n",
            "      else:\n",
            "        Predicted value: -0.56\n",
            "    else:\n",
            "      if Feature 9 <= 1.85:\n",
            "        if Feature 6 <= -0.95:\n",
            "          Predicted value: 1.23\n",
            "        else:\n",
            "          Predicted value: 0.57\n",
            "      else:\n",
            "        if Feature 6 <= -0.37:\n",
            "          Predicted value: 1.70\n",
            "        else:\n",
            "          Predicted value: 0.70\n",
            "\n",
            "\n",
            "Decision Tree 5:\n",
            "if Feature 7 <= -0.10:\n",
            "  if Feature 8 <= -0.92:\n",
            "    if Feature 9 <= -1.20:\n",
            "      if Feature 5 <= -1.72:\n",
            "        Predicted value: -0.12\n",
            "      else:\n",
            "        if Feature 1 <= 0.06:\n",
            "          Predicted value: -0.43\n",
            "        else:\n",
            "          Predicted value: -1.06\n",
            "    else:\n",
            "      if Feature 0 <= 0.69:\n",
            "        if Feature 6 <= 0.02:\n",
            "          Predicted value: -0.89\n",
            "        else:\n",
            "          Predicted value: -1.25\n",
            "      else:\n",
            "        if Feature 0 <= 1.03:\n",
            "          Predicted value: -0.17\n",
            "        else:\n",
            "          Predicted value: -1.02\n",
            "  else:\n",
            "    if Feature 8 <= 0.35:\n",
            "      if Feature 2 <= 0.22:\n",
            "        if Feature 4 <= -1.10:\n",
            "          Predicted value: -0.05\n",
            "        else:\n",
            "          Predicted value: -0.63\n",
            "      else:\n",
            "        if Feature 4 <= 0.60:\n",
            "          Predicted value: 0.48\n",
            "        else:\n",
            "          Predicted value: 1.59\n",
            "    else:\n",
            "      if Feature 4 <= 1.12:\n",
            "        if Feature 2 <= 0.16:\n",
            "          Predicted value: 0.62\n",
            "        else:\n",
            "          Predicted value: 1.30\n",
            "      else:\n",
            "        Predicted value: -0.96\n",
            "else:\n",
            "  if Feature 2 <= -0.18:\n",
            "    if Feature 7 <= 1.35:\n",
            "      if Feature 0 <= 0.92:\n",
            "        if Feature 1 <= 0.06:\n",
            "          Predicted value: -0.29\n",
            "        else:\n",
            "          Predicted value: -0.80\n",
            "      else:\n",
            "        if Feature 6 <= -0.99:\n",
            "          Predicted value: -0.51\n",
            "        else:\n",
            "          Predicted value: 0.56\n",
            "    else:\n",
            "      if Feature 4 <= 1.40:\n",
            "        if Feature 0 <= 0.80:\n",
            "          Predicted value: -0.01\n",
            "        else:\n",
            "          Predicted value: -0.82\n",
            "      else:\n",
            "        if Feature 8 <= 0.88:\n",
            "          Predicted value: 1.31\n",
            "        else:\n",
            "          Predicted value: 1.15\n",
            "  else:\n",
            "    if Feature 0 <= -1.53:\n",
            "      if Feature 6 <= -0.49:\n",
            "        if Feature 8 <= 1.14:\n",
            "          Predicted value: -0.20\n",
            "        else:\n",
            "          Predicted value: -1.09\n",
            "      else:\n",
            "        Predicted value: 1.22\n",
            "    else:\n",
            "      if Feature 9 <= 0.80:\n",
            "        if Feature 3 <= -1.07:\n",
            "          Predicted value: -0.56\n",
            "        else:\n",
            "          Predicted value: 0.63\n",
            "      else:\n",
            "        if Feature 0 <= 1.64:\n",
            "          Predicted value: 1.48\n",
            "        else:\n",
            "          Predicted value: 0.63\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Hyperparameter Tuning (From-Scratch Implementation): [5 Marks]\n",
        "* Use GridSearchCV from scikit-learn to find the optimal number of trees for your from-scratch implementation.\n",
        "* Evaluate the model with the best number of trees on the test set and report the Mean Squared Error (MSE) and the OOB error.\n"
      ],
      "metadata": {
        "id": "cu2DcSgd91J7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WT135u0S-NSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Scikit-learn Implementation: [5 Marks]\n",
        "* Use RandomForestRegressor from scikit-learn to train a model on the diabetes dataset.\n",
        "* Using cross-validation find the optimal number of trees for the scikit-learn implementation.\n",
        "* Evaluate the model with the best number of trees on the test set and report the MSE and OOB.\n"
      ],
      "metadata": {
        "id": "3J06RHYw95bT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qcb6viSz-ORG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "6. Comparison and Visualization: [2 Marks]\n",
        "* Compare the performance (MSE) of your from-scratch implementation with the scikit-learn implementation.\n",
        "* Create scatter plots to visualize:\n",
        "  * Predicted values vs. true values for your from-scratch implementation.\n",
        "  * Predicted values vs. true values for the scikit-learn implementation.\n",
        "* Display these plots side-by-side for easy comparison."
      ],
      "metadata": {
        "id": "Ekpj-z0L-FH1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdk62RbZ-PEd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}